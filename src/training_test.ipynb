{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook created to train the logistic regression model without having to reload the dataset every time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7651970a6e707"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Because for some reason, ucimlrepository takes an weirdly long time to load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9707a8373c00428e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "from numpy import floating as fl, float32 as f32, float64 as f64, int32 as i32\n",
    "from numpy.typing import NDArray\n",
    "from pprint import pprint\n",
    "\n",
    "# Iris dataset\n",
    "DATASET_ID = 53\n",
    "\n",
    "iris = fetch_ucirepo(id=DATASET_ID)  # fetch dataset\n",
    "assert iris.data is not None\n",
    "\n",
    "DATA: DataFrame = iris.data.original\n",
    "LAB_NAME: str = iris.data[\"headers\"][-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# FEAT, FEAT_test, y_train, y_test = train_test_split(iris.data.features, DATA[LAB_NAME], test_size=0.3, random_state=np.random.randint(0, 100))\n",
    "FEAT, FEAT_test, y_train, y_test = train_test_split(iris.data.features, DATA[LAB_NAME], test_size=0.3, random_state=42)\n",
    "\n",
    "DATA_train = FEAT.copy(deep=True)\n",
    "DATA_train[\"class\"] = y_train\n",
    "\n",
    "DATA_test = FEAT_test.copy(deep=True)\n",
    "DATA_test[\"class\"] = y_test\n",
    "\n",
    "# FEAT: DataFrame = X_train\n",
    "# FEAT_test: DataFrame = X_test\n",
    "\n",
    "LABELS_STR: DataFrame = DATA[LAB_NAME]  # class value as string\n",
    "# LABELS_STR_test: DataFrame = DATA_test[LAB_NAME]  # type: ignore\n",
    "\n",
    "lab_values = LABELS_STR.unique()\n",
    "# lab_values_test = LLAB_IDX_VALABELS_STR_test.unique()\n",
    "\n",
    "LAB_IDX_VAL: dict[int, str] = dict(zip(range(len(lab_values)), lab_values))  # class index, class value\n",
    "LAB_VAL_IDX: dict[str, int] = {v: k for k, v in LAB_IDX_VAL.items()}  # class value, class index\n",
    "\n",
    "LABELS: NDArray = np.array([LAB_VAL_IDX[class_value] for class_value in y_train])\n",
    "LABELS_test: NDArray = np.array([LAB_VAL_IDX[class_value] for class_value in y_test])\n",
    "COL_NAMES = list(FEAT.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:41.355714119Z",
     "start_time": "2024-01-09T15:03:39.779151288Z"
    }
   },
   "id": "863f7e3ebaac994f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length  sepal width  petal length  petal width\n81            5.5          2.4           3.7          1.0\n133           6.3          2.8           5.1          1.5\n137           6.4          3.1           5.5          1.8\n75            6.6          3.0           4.4          1.4\n109           7.2          3.6           6.1          2.5\n..            ...          ...           ...          ...\n71            6.1          2.8           4.0          1.3\n106           4.9          2.5           4.5          1.7\n14            5.8          4.0           1.2          0.2\n92            5.8          2.6           4.0          1.2\n102           7.1          3.0           5.9          2.1\n\n[105 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length</th>\n      <th>sepal width</th>\n      <th>petal length</th>\n      <th>petal width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>81</th>\n      <td>5.5</td>\n      <td>2.4</td>\n      <td>3.7</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>6.3</td>\n      <td>2.8</td>\n      <td>5.1</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>6.4</td>\n      <td>3.1</td>\n      <td>5.5</td>\n      <td>1.8</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>6.6</td>\n      <td>3.0</td>\n      <td>4.4</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>7.2</td>\n      <td>3.6</td>\n      <td>6.1</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>6.1</td>\n      <td>2.8</td>\n      <td>4.0</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>4.9</td>\n      <td>2.5</td>\n      <td>4.5</td>\n      <td>1.7</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5.8</td>\n      <td>4.0</td>\n      <td>1.2</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>5.8</td>\n      <td>2.6</td>\n      <td>4.0</td>\n      <td>1.2</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>7.1</td>\n      <td>3.0</td>\n      <td>5.9</td>\n      <td>2.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>105 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEAT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:45.366835416Z",
     "start_time": "2024-01-09T15:03:45.359032625Z"
    }
   },
   "id": "2c39c158c925f16a",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "`gradient_descent.py`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7140fc85d73feb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def grad_desc_ml(\n",
    "        features: NDArray, labels: NDArray, df, w: NDArray, b: float, alpha: float, num_iters: int\n",
    ") -> tuple[NDArray, fl]:\n",
    "    \"\"\"Same gradient descent `gradient_desent` method, but that takes `features` (X) and `labels` (y)\n",
    "    as additional parameters, since they're obviously going to be need for any kind of learning whatsoever.\n",
    "    Parameters\n",
    "    ----------\n",
    "    `features` : NDArray\n",
    "        Samples / features.\n",
    "    `labels` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `df`: function\n",
    "        derivative function (i.e. gradient)\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    `alpha`: float\n",
    "        define how the function will converge. Values too big will give bad results and values too small won't converge or will converge too slowly\n",
    "    `num_iters`: Number of iterations\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal vector for the initial configuration and parameters\"\"\"\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        grad_w, grad_b = df(features, labels, w, b)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "    return w, b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:48.325466136Z",
     "start_time": "2024-01-09T15:03:48.323600937Z"
    }
   },
   "id": "97d4dc32e4b35c04",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "`log_reg.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53919941aa5dfa84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def z(X: NDArray, w: NDArray, b: fl) -> fl:\n",
    "#     \"\"\"\n",
    "#     Returns\n",
    "#     -------\n",
    "#     ``np.dot(X, w) + b``: `float` or `NDArray[float]` (i.e. `floating`)\n",
    "#     Notes\n",
    "#     -----\n",
    "#     `w` and `X` can be interchanged e.g. `z(w, X, b)`, it won't give\n",
    "#     the same result (in general) but as long as matrix multiplication dimensions\n",
    "#     are respected, it will work.\"\"\"\n",
    "#     return np.dot(X, w) + b\n",
    "# \n",
    "# \n",
    "# def sigmoid(z: fl) -> fl:\n",
    "#     \"\"\" Returns\n",
    "#     -----------\n",
    "#     1 / (1 + exp(-z))\"\"\"\n",
    "#     return 1 / (1 + np.exp(-z))\n",
    "# \n",
    "# \n",
    "# def norm(X: NDArray):\n",
    "#     return (X - np.mean(X)) / np.std(X)\n",
    "# \n",
    "# \n",
    "# def grad(X: NDArray, y: NDArray, w: NDArray, b: fl):\n",
    "#     \"\"\"Computes (vectorized) the gradient of the log loss function w.r.t \"w\" and \"b\" for the current iteration.\n",
    "#     It is used in the gradient descent algorithm.\n",
    "# \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     `X` : NDArray\n",
    "#         Samples / features.\n",
    "#     `y` : NDArray\n",
    "#         labels / class associated to each sample.\n",
    "#     `w` : NDArray\n",
    "#         weights vector.\n",
    "#     `b` : fl (float or NDArray[float])\n",
    "#         bias\n",
    "#     Returns\n",
    "#     -------\n",
    "#     (dw, db) :\n",
    "#         The gradient of the log loss function w.r.t \"w\" and \"b\".\"\"\"\n",
    "# \n",
    "#     predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "#     errors = y - predictions  # Difference between actual and predicted values\n",
    "#     db = -np.sum(errors)  # Vectorized computation of db component\n",
    "# \n",
    "#     X_sum_over_rows = np.sum(X, axis=1)  # Sum over rows of X\n",
    "#     dw = -np.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "# \n",
    "#     return dw, db\n",
    "\n",
    "\n",
    "# def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: fl, n_it: int, lr: float) -> tuple[NDArray, fl]:\n",
    "#     \"\"\"\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     `X` : NDArray\n",
    "#         Samples / features.\n",
    "#     `y` : NDArray\n",
    "#         labels / class associated to each sample.\n",
    "#     `w` : NDArray\n",
    "#         initial weight vector.\n",
    "#     `b` : fl (float or NDArray[float])\n",
    "#         inital bias\n",
    "#     `n_it` : int\n",
    "#         iterations number\n",
    "#     `lr` : float\n",
    "#         learning rate\n",
    "#     Returns\n",
    "#     -------\n",
    "#         Trained (weight vector, bias) with gradient descent that minimize the log loss function.\"\"\"\n",
    "#     return grad_desc_ml(X, y, grad, w, b, lr, n_it)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:01:56.440319936Z",
     "start_time": "2024-01-09T15:01:56.400774237Z"
    }
   },
   "id": "1b681218d41ac429",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression but with CuPy (Nvidia / Cuda)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596326ec80a95575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(data, predicted_values):\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "    \"\"\" This function calculates the performance metrics for each class in a binary classification problem.\n",
    "        The metrics calculated are Precision, Recall, and F1 Score.\n",
    "        :param data: (DataFrame): The DataFrame containing the actual labels.\n",
    "        :param predicted_values: (list): The list containing the predicted labels.\n",
    "        :return: dict: A dictionary containing the performance metrics for each class.\"\"\"\n",
    "    y_true, y_pred = data, predicted_values\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "    return {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_score': float(f1)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:51.121088796Z",
     "start_time": "2024-01-09T15:03:51.119007040Z"
    }
   },
   "id": "8b450001d36befda",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cupy import ndarray as CPArray\n",
    "from numpy.random import rand, randint\n",
    "\n",
    "\n",
    "def z(w: CPArray, X: CPArray, b: float) -> CPArray:\n",
    "    \"\"\":return: ``cp.dot(X, w) + b``.\n",
    "    i.e. float or CPArray[float] (i.e. cupy.ndarray)\n",
    "    NOTE: `w` and `X` can be interchanged e.g. (`z(X, w, b)), it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return cp.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z): return 1 / (1 + cp.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: CPArray): return (X - cp.mean(X)) / cp.std(X)\n",
    "\n",
    "\n",
    "def grad_gpu(X: CPArray, y: CPArray, w: CPArray, b: float):\n",
    "    \"\"\":return: (dw, db). i.e. Computes aforementioned derivatives w.r.t \"w\" and \"b\". \n",
    "    (on gpu. X, y, w, b are `cupy.ndarray` shortened to `CPArray`)\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -cp.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = cp.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -cp.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: float, n_it: int, lr: float) -> tuple[NDArray, float]:\n",
    "    \"\"\"\n",
    "    :param X: Feature matrix (covariables)\n",
    "    :param y: Label vector\n",
    "    :param w: initial weight vector\n",
    "    :param b:  initial bias\n",
    "    :param n_it: iterations number\n",
    "    :param lr: learning rate\n",
    "    :return: Trained weight vector and bias to minimize by gradient descent.\n",
    "    \"\"\"\n",
    "    X, y, w = map(cp.array, (X, y, w))\n",
    "    # for _ in range(n_it):\n",
    "    #     grad_w, grad_b = grad(X, y, w, b)\n",
    "    #     w -= lr * grad_w\n",
    "    #     b -= lr * grad_b\n",
    "    # return w, b\n",
    "    return grad_desc_ml(X, y, grad_gpu, w, b, lr, n_it)\n",
    "\n",
    "\n",
    "def predict_log_reg(X: NDArray, w: NDArray, b):\n",
    "    \"\"\" Predict the class labels for a set of examples X using logistic regression parameters w and b.\n",
    "    :param X: The input features. 2D Matrix NDArray\n",
    "    :param w: The weights of the logistic regression model. Vector NDArray\n",
    "    :param b: The bias of the logistic regression model. float\n",
    "    :return: Vector of predicted class labels (0 or 1) for each example in X. Vector NDArray\n",
    "    \"\"\"\n",
    "    X, w = map(cp.array, (X, w))\n",
    "    # return i32(sigmoid(z(w, X, b)).get() >= 0.5)\n",
    "    # print(sigmoid(z(w, X, b)))\n",
    "    # predicted= sigmoid((z(w, X, b))).get()\n",
    "    predicted = sigmoid(norm(z(w, X, b))).get()\n",
    "    class_nb = len(LAB_VAL_IDX)\n",
    "    return i32(predicted * class_nb)  # returns k-1 if val < (k/class_nb) i.e. 0 if < 1/3, 1 if < 2/3...\n",
    "\n",
    "\n",
    "def pred_compute(X_test, Y_test, w, b):\n",
    "    # if DataFrame or Series is passed, convert to numpy array\n",
    "    if type(X_test) != np.ndarray: X_test = np.asarray(X_test)\n",
    "    # for i in range(X_test.shape[1]): X_test[:, i] = norm(X_test[:, i])\n",
    "    # X_test[:, :] = np.apply_along_axis(norm, 0, X_test[:, :])\n",
    "    predicted_val_logreg = predict_log_reg(X_test, w, b)\n",
    "    metrics = compute_metrics(Y_test, predicted_val_logreg)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def test_train_gpu(m, n):\n",
    "    X, y, w, b = rand(m, n), rand(m), rand(n), rand()\n",
    "    n_it, lr = 100, 0.03\n",
    "    w, b = train_log_reg(X, y, w, b, n_it, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:52.619439412Z",
     "start_time": "2024-01-09T15:03:52.497948326Z"
    }
   },
   "id": "8fd76d66732d0098",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#class_nb = len(LAB_VAL_IDX)\n",
    "#thresholds = np.linspace(1 / class_nb, 1, class_nb - 1, endpoint=False)\n",
    "#val = 0.34\n",
    "#idx = 0\n",
    "## while idx < (class_nb - 1) and val > thresholds[idx]: idx += 1\n",
    "#idx = int(val * class_nb)\n",
    "#print(thresholds)\n",
    "#print(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:01:56.618724997Z",
     "start_time": "2024-01-09T15:01:56.607147364Z"
    }
   },
   "id": "884256c8bb7cad8",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_compute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#_w, _b = np.array([0.57880481, 0.93467781, 0.80771699, 0.89825272]), 0.03075745284540543\u001B[39;00m\n\u001B[1;32m      2\u001B[0m w, b \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.53452349\u001B[39m, \u001B[38;5;241m0.36463584\u001B[39m, \u001B[38;5;241m1.16132476\u001B[39m, \u001B[38;5;241m1.08204578\u001B[39m], \u001B[38;5;241m0.45146791\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mpred_compute\u001B[49m(FEAT_test, LABELS_test, w, b)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# predict_log_reg(FEAT_test, w, b)\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pred_compute' is not defined"
     ]
    }
   ],
   "source": [
    "#_w, _b = np.array([0.57880481, 0.93467781, 0.80771699, 0.89825272]), 0.03075745284540543\n",
    "w, b = [0.53452349, 0.36463584, 1.16132476, 1.08204578], 0.45146791\n",
    "pred_compute(FEAT_test, LABELS_test, w, b)\n",
    "\n",
    "# predict_log_reg(FEAT_test, w, b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:04:00.383335255Z",
     "start_time": "2024-01-09T15:04:00.285282150Z"
    }
   },
   "id": "278a7f868d844f9",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": "    0\n0   1\n1   1\n2   2\n3   1\n4   1\n5   0\n6   1\n7   2\n8   1\n9   1\n10  2\n11  0\n12  0\n13  0\n14  0\n15  2\n16  2\n17  1\n18  1\n19  2\n20  0\n21  1\n22  0\n23  2\n24  2\n25  2\n26  2\n27  2\n28  0\n29  0\n30  0\n31  1\n32  1\n33  0\n34  0\n35  1\n36  1\n37  0\n38  0\n39  0\n40  1\n41  1\n42  2\n43  0\n44  0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exit(0)\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "m, n = FEAT.shape\n",
    "init_w = np.random.rand(n)\n",
    "init_b = np.random.rand()\n",
    "n_it, lr = 1000, 1e-5\n",
    "\n",
    "# init_w = np.array([0.561324, 0.65314283, 0.35009123, 0.52246315, 0.21069777, 0.10848934, -0.23922914, 0.36291324, 0.27269732, 0.6531454, 0.09283825, 0.26135069, -0.04080029])\n",
    "# init_b =  0.688047328800096\n",
    "w = np.array([ 0.81480015,  0.15855598, -0.06485859,  0.23753591,  0.42363255, -0.07836012,  0.49241348,  0.69423898,  0.47395861, -0.13917395, 0.38662914, -0.04201902, -0.10299166])\n",
    "b = 0.82586609\n",
    "# w, b = init_w, init_b\n",
    "w, b = train_log_reg(FEAT.to_numpy(), LABELS, init_w, init_b, n_it, lr)\n",
    "predicted_val_logreg = predict_log_reg(FEAT_test.to_numpy(), w, b)\n",
    "\n",
    "score = metrics.f1_score(LABELS_test, predicted_val_logreg, average=\"micro\")\n",
    "# pprint(w)\n",
    "# pprint(b)\n",
    "print(\"score:\", score)\n",
    "pd.DataFrame(predicted_val_logreg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:03:57.220818081Z",
     "start_time": "2024-01-09T15:03:57.028796707Z"
    }
   },
   "id": "78464500be88595e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m, n = FEAT.shape\n",
    "init_w = np.random.rand(n)\n",
    "init_b = np.random.rand()\n",
    "n_it, lr = 850, 1e-07\n",
    "\n",
    "\n",
    "# lr = 3.162277660168379e-06\n",
    "# init_w, init_b = [0.53452349, 0.36463584, 1.16132476, 1.08204578], 0.45146791\n",
    "# init_w, init_b = np.array([0.57880481, 0.93467781, 0.80771699, 0.89825272]), 0.03075745284540543\n",
    "# init_w, init_b = [2.72002667, 3.07589967, 2.94893885, 3.03947458], 0.08176500616680844\n",
    "# init_w, init_b = np.array([0.1911183 , 0.93876337, 0.98824842, 0.7948038]), 0.83589593\n",
    "# w, b = np.array([0.57880481, 0.93467781, 0.80771699, 0.89825272]), 0.03075745284540543\n",
    "# w, b = train_log_reg(FEAT.to_numpy(), LABELS, init_w, init_b, n_it, lr)\n",
    "# w, b = init_w, init_b\n",
    "# print(\"\\tWeights, \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tbias\\n\", w, b)\n",
    "def max_lr():\n",
    "    max_f1_scores = []\n",
    "    for expo in np.arange(0, 10, step=0.25):\n",
    "        lr = 10 ** (-expo)\n",
    "        X = np.asarray(FEAT.copy(deep=True))\n",
    "        # X = np.apply_along_axis(norm, 0, np.asarray(FEAT.copy(deep=True)))\n",
    "        w, b = train_log_reg(X, LABELS, init_w, init_b, n_it, lr)\n",
    "        score = pred_compute(FEAT_test.to_numpy(), LABELS_test, w, b)\n",
    "        max_f1_scores.append((score[\"f1_score\"], lr, w, b))\n",
    "    return max(max_f1_scores, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def max_it():\n",
    "    max_f1_scores = []\n",
    "    for iter in range(100, 5000, 25):\n",
    "        w, b = train_log_reg(FEAT.to_numpy(), LABELS, init_w, init_b, iter, lr)\n",
    "        score = pred_compute(FEAT_test.to_numpy(), LABELS_test, w, b)\n",
    "        max_f1_scores.append((score[\"f1_score\"], iter))\n",
    "    return max(max_f1_scores, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "# max_lr()\n",
    "# pred_compute(FEAT_test.to_numpy(), LABELS_test, w, b)\n",
    "# max_it()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T15:01:57.037358342Z",
     "start_time": "2024-01-09T15:01:56.966484547Z"
    }
   },
   "id": "b916e84f34b77584",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
