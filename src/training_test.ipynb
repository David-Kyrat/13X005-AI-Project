{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook created to train the logistic regression model without having to reload the dataset every time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7651970a6e707"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Because for some reason, ucimlrepository takes an weirdly long time to load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9707a8373c00428e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "from numpy import floating as fl, float32 as f32, float64 as f64, int32 as i32\n",
    "from numpy.typing import NDArray\n",
    "from pprint import pprint\n",
    "\n",
    "# Iris dataset\n",
    "DATASET_ID = 53\n",
    "\n",
    "iris = fetch_ucirepo(id=DATASET_ID)  # fetch dataset\n",
    "assert iris.data is not None\n",
    "\n",
    "DATA: DataFrame = iris.data.original\n",
    "\n",
    "FEAT: DataFrame = iris.data.features\n",
    "LAB_NAME: str = iris.data[\"headers\"][-1]\n",
    "LABELS_STR: DataFrame = DATA[LAB_NAME]  # type: ignore\n",
    "lab_values  = LABELS_STR.unique()\n",
    "\n",
    "LAB_IDX_VAL: dict[int, str] = dict(zip(range(len(lab_values)), lab_values))\n",
    "LAB_VAL_IDX: dict[str, int] = dict(zip(lab_values, range(len(lab_values))))\n",
    "    \n",
    "LABELS: NDArray[int] = np.array([LAB_VAL_IDX[class_value] for class_value in LABELS_STR])\n",
    "COL_NAMES = list(FEAT.columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.391051507Z",
     "start_time": "2024-01-08T22:28:18.836942790Z"
    }
   },
   "id": "863f7e3ebaac994f",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "`gradient_descent.py`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7140fc85d73feb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def grad_desc_ml(\n",
    "        features: NDArray, labels: NDArray, df, w: NDArray, b: fl, alpha: float, num_iters: int\n",
    ") -> tuple[NDArray, fl]:\n",
    "    \"\"\"Same gradient descent `gradient_desent` method, but that takes `features` (X) and `labels` (y)\n",
    "    as additional parameters, since they're obviously going to be need for any kind of learning whatsoever.\n",
    "    Parameters\n",
    "    ----------\n",
    "    `features` : NDArray\n",
    "        Samples / features.\n",
    "    `labels` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `df`: function\n",
    "        derivative function (i.e. gradient)\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    `alpha`: float\n",
    "        define how the function will converge. Values too big will give bad results and values too small won't converge or will converge too slowly\n",
    "    `num_iters`: Number of iterations\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal vector for the initial configuration and parameters\"\"\"\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        grad_w, grad_b = df(features, labels, w, b)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "    return w, b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.433236776Z",
     "start_time": "2024-01-08T22:28:20.432777909Z"
    }
   },
   "id": "97d4dc32e4b35c04",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "`log_reg.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53919941aa5dfa84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def z(X: NDArray, w: NDArray, b: fl) -> fl:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    ``np.dot(X, w) + b``: `float` or `NDArray[float]` (i.e. `floating`)\n",
    "    Notes\n",
    "    -----\n",
    "    `w` and `X` can be interchanged e.g. `z(w, X, b)`, it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z: fl) -> fl:\n",
    "    \"\"\" Returns\n",
    "    -----------\n",
    "    1 / (1 + exp(-z))\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: NDArray):\n",
    "    return (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "\n",
    "def grad(X: NDArray, y: NDArray, w: NDArray, b: fl):\n",
    "    \"\"\"Computes (vectorized) the gradient of the log loss function w.r.t \"w\" and \"b\" for the current iteration.\n",
    "    It is used in the gradient descent algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    Returns\n",
    "    -------\n",
    "    (dw, db) :\n",
    "        The gradient of the log loss function w.r.t \"w\" and \"b\".\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -np.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = np.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -np.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: fl, n_it: int, lr: float) -> tuple[NDArray, fl]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        initial weight vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        inital bias\n",
    "    `n_it` : int\n",
    "        iterations number\n",
    "    `lr` : float\n",
    "        learning rate\n",
    "    Returns\n",
    "    -------\n",
    "        Trained (weight vector, bias) with gradient descent that minimize the log loss function.\"\"\"\n",
    "    return grad_desc_ml(X, y, grad, w, b, lr, n_it)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.433472884Z",
     "start_time": "2024-01-08T22:28:20.432935866Z"
    }
   },
   "id": "1b681218d41ac429",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression but with CuPy (Nvidia / Cuda)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596326ec80a95575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(data, predicted_values):\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "    \"\"\" This function calculates the performance metrics for each class in a binary classification problem.\n",
    "        The metrics calculated are Precision, Recall, and F1 Score.\n",
    "        :param data: (DataFrame): The DataFrame containing the actual labels.\n",
    "        :param predicted_values: (list): The list containing the predicted labels.\n",
    "        :return: dict: A dictionary containing the performance metrics for each class.\"\"\"\n",
    "    y_true, y_pred = data, predicted_values\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_score': float(f1)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.444777115Z",
     "start_time": "2024-01-08T22:28:20.433014834Z"
    }
   },
   "id": "8b450001d36befda",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cupy import ndarray as CPArray\n",
    "from numpy.random import rand, randint\n",
    "\n",
    "\n",
    "def z(w: CPArray, X: CPArray, b: float) -> CPArray:\n",
    "    \"\"\":return: ``cp.dot(X, w) + b``.\n",
    "    i.e. float or CPArray[float] (i.e. cupy.ndarray)\n",
    "    NOTE: `w` and `X` can be interchanged e.g. (`z(X, w, b)), it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return cp.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z): return 1 / (1 + cp.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: CPArray): return (X - cp.mean(X)) / cp.std(X)\n",
    "\n",
    "\n",
    "def grad(X: CPArray, y: CPArray, w: CPArray, b: float):\n",
    "    \"\"\":return: (dw, db). i.e. Computes aforementioned derivatives w.r.t \"w\" and \"b\". \n",
    "    (on gpu. X, y, w, b are `cupy.ndarray` shortened to `CPArray`)\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -cp.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = cp.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -cp.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# because shorter names for function and variable, while keeping the function name as asked from the exercise\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: float, n_it: int, lr: float) -> tuple[NDArray, float]:\n",
    "    \"\"\"\n",
    "    :param X: Feature matrix (covariables)\n",
    "    :param y: Label vector\n",
    "    :param w: initial weight vector\n",
    "    :param b:  initial bias\n",
    "    :param n_it: iterations number\n",
    "    :param lr: learning rate\n",
    "    :return: Trained weight vector and bias to minimize by gradient descent.\n",
    "    \"\"\"\n",
    "    X, y, w = map(cp.array, (X, y, w))\n",
    "    for _ in range(n_it):\n",
    "        grad_w, grad_b = grad(X, y, w, b)\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def predict_log_reg(X: NDArray, w: NDArray, b):\n",
    "    \"\"\" Predict the class labels for a set of examples X using logistic regression parameters w and b.\n",
    "    :param X: The input features. 2D Matrix NDArray\n",
    "    :param w: The weights of the logistic regression model. Vector NDArray\n",
    "    :param b: The bias of the logistic regression model. float\n",
    "    :return: Vector of predicted class labels (0 or 1) for each example in X. Vector NDArray\n",
    "    \"\"\"\n",
    "    X, w = map(cp.array, (X, w))\n",
    "    return i32(sigmoid(z(w, X, b)).get() >= 0.5)\n",
    "\n",
    "\n",
    "def test_train_gpu(m, n):\n",
    "    X, y, w, b = rand(m, n), rand(m), rand(n), rand()\n",
    "    n_it, lr = 100, 0.03\n",
    "    w, b = train_log_reg(X, y, w, b, n_it, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:42.949813842Z",
     "start_time": "2024-01-08T22:28:42.943155084Z"
    }
   },
   "id": "8fd76d66732d0098",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([4.30618697, 3.66449773, 4.13295625, 4.25646106]), array(0.51899835))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = FEAT.shape\n",
    "init_w = np.random.rand(n)\n",
    "init_b = np.random.rand()\n",
    "n_it, lr = 1000, 1e-5\n",
    "\n",
    "train_log_reg(FEAT.to_numpy(), LABELS, init_w, init_b, n_it, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:43:16.014666235Z",
     "start_time": "2024-01-08T22:43:15.852462466Z"
    }
   },
   "id": "b916e84f34b77584",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pred_compute(data_test, w, b):\n",
    "    X_test = np.array([data_test.Gender_Female, data_test.Age, data_test.EstimatedSalary]).T  # features\n",
    "    X_test[:, 1:3] = np.apply_along_axis(norm, 0, X_test[:, 1:3])\n",
    "    predicted_val_logreg = predict_log_reg(X_test, w, b)\n",
    "    metrics = compute_metrics(predicted[\"actual\"], predicted_val_logreg)\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.983034294Z",
     "start_time": "2024-01-08T22:28:20.937658564Z"
    }
   },
   "id": "b0b4441bfe52cd68",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def maximize_train_param(tries: int):\n",
    "    params = []\n",
    "    # w = np.array([0.13276234, 0.24566002, - 0.00970713])\n",
    "    w = np.array([0.13017195, 0.24306963, -0.01229752])\n",
    "    n_it = 3900\n",
    "    # for n_it in np.linspace(3732, 3760, tries, dtype=int):\n",
    "    print(pred_compute(data_test, w, 0)[\"f1_score\"])\n",
    "    for lr in np.linspace(1e-9 * 0.01, 1e-9 * 2, tries, dtype=f64):\n",
    "        # w = np.random.uniform(0, 0.5, size=3)\n",
    "        b = 0.\n",
    "        w, b = train_log_reg(X, labels, np.array([0.13017195, 0.24306963, -0.01229752]), 0., n_it, lr)\n",
    "        f1_score_ = pred_compute(data_test, w, b)[\"f1_score\"]\n",
    "        params.append((w, n_it, lr, f1_score_))\n",
    "        print(w, lr, \"\\nf1_score:\", f1_score_)\n",
    "        print(\"______\")\n",
    "\n",
    "    return max(params, key=lambda x: x[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T22:28:20.991938802Z",
     "start_time": "2024-01-08T22:28:20.978866181Z"
    }
   },
   "id": "aa22b58e0d6cd603",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
