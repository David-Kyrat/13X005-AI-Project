{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook created to train the logistic regression model without having to reload the dataset every time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7651970a6e707"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Because for some reason, ucimlrepository takes an weirdly long time to load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9707a8373c00428e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Iris dataset\n",
    "DATASET_ID = 53\n",
    "\n",
    "iris = fetch_ucirepo(id=DATASET_ID)  # fetch dataset\n",
    "assert iris.data is not None\n",
    "\n",
    "DATA: DataFrame = iris.data.original\n",
    "\n",
    "FEAT: DataFrame = iris.data.features\n",
    "LAB_NAME: str = iris.data[\"headers\"][-1]\n",
    "LABELS: DataFrame = DATA[LAB_NAME]  # type: ignore\n",
    "COL_NAMES = list(FEAT.columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:21:41.508002456Z",
     "start_time": "2024-01-08T21:21:40.004814347Z"
    }
   },
   "id": "863f7e3ebaac994f",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "`gradient_descent.py`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7140fc85d73feb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plot_util\n",
    "from numpy import cos, pi, sin\n",
    "from numpy import floating as fl\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "def gradient_descent(df, params: NDArray, alpha: float, num_iters: int) -> NDArray:\n",
    "    \"\"\"This function implements the gradient descent. It iteratively computes the optimal parameters that minimize the given function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `df`: function\n",
    "        derivative function (i.e. gradient)\n",
    "    `params`: NDArray\n",
    "        Initial vector of parameters to optimize\n",
    "    `alpha`: float\n",
    "        define how the function will converge. Values too big will give bad results and values too small won't converge or converge will too slowly\n",
    "    `num_iters`: int\n",
    "        Number of iterations\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal vector for the initial configuration and parameters\"\"\"\n",
    "    for _ in range(num_iters):\n",
    "        params -= alpha * df(params)\n",
    "    return params\n",
    "\n",
    "\n",
    "def grad_desc_ml(\n",
    "        features: NDArray, labels: NDArray, df, w: NDArray, b: fl, alpha: float, num_iters: int\n",
    ") -> tuple[NDArray, fl]:\n",
    "    \"\"\"Same gradient descent `gradient_desent` method, but that takes `features` (X) and `labels` (y)\n",
    "    as additional parameters, since they're obviously going to be need for any kind of learning whatsoever.\n",
    "    Parameters\n",
    "    ----------\n",
    "    `features` : NDArray\n",
    "        Samples / features.\n",
    "    `labels` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `df`: function\n",
    "        derivative function (i.e. gradient)\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    `alpha`: float\n",
    "        define how the function will converge. Values too big will give bad results and values too small won't converge or will converge too slowly\n",
    "    `num_iters`: Number of iterations\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal vector for the initial configuration and parameters\"\"\"\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        grad_w, grad_b = df(features, labels, w, b)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "    return w, b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:21:41.649328689Z",
     "start_time": "2024-01-08T21:21:41.507829657Z"
    }
   },
   "id": "97d4dc32e4b35c04",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes\n",
    "`naive_bayes.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299420af4078581"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Any\n",
    "\n",
    "# NB: floating is any (numpy) floating type NDArray or not\n",
    "from numpy import float32 as f32, floating as fl\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "from pandas import DataFrame\n",
    "\n",
    "def normal_pdf(mean: fl, std: fl):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `mean` : float or NDArray of float\n",
    "        The mean (μ) of the normal distribution.\n",
    "    `std : float or NDArray of float\n",
    "        The standard deviation (σ) of the normal distribution.\n",
    "    Returns\n",
    "    -------\n",
    "    A lambda function representing the normal distribution's PDF,\n",
    "    i.e.  (1 / (σ * sqrt(2π))) * exp(-((x - μ)² / (2σ²))).\"\"\"\n",
    "    return lambda x: (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-((x - mean) ** 2) / (2 * std**2))\n",
    "\n",
    "\n",
    "def get_distrib_parameters(data: DataFrame, feature_names: list[str], labels: DataFrame) -> dict[Any, list[tuple[fl, fl]]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `data` : The dataset.\n",
    "    `feature_names` : The names of the features to extract the normal parameters from.\n",
    "    `labels` : Labels to extract the different values from (will be the keys of the return dict)\n",
    "    Returns\n",
    "    -------\n",
    "    Parameters for each distribution of each feature feature for each class.\n",
    "    i.e. a dictionary {class: [(mean_i, std_i), ...]} for each feature i.\"\"\"\n",
    "    classes = labels.unique()\n",
    "    out: dict[Any, list[tuple[fl, fl]]] = {}\n",
    "    for classv in classes:\n",
    "        out_classv = []  # list of (mean, std) for each feature by class value\n",
    "        data_c = data[labels == classv]  # data for current class\n",
    "        for feature in feature_names:\n",
    "            feat = data_c[feature]\n",
    "            mean, std = feat.mean(), feat.std()\n",
    "            out_classv.append((f32(mean), f32(std)))\n",
    "        out[classv] = out_classv\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict_bayes(x: NDArray, params_by_class: dict[Any, list[tuple[fl, fl]]]) -> Any:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `x` : The sample to predict.\n",
    "    `params_by_class` : The parameters of the normal distribution of each feature for each class.\n",
    "    Returns\n",
    "    -------\n",
    "    The predicted class for the sample x.\"\"\"\n",
    "    probs = {}\n",
    "    if type(x) is not np.ndarray:\n",
    "        x = np.asarray(x)\n",
    "\n",
    "    for class_value, params in params_by_class.items():\n",
    "        probs[class_value] = 1\n",
    "        for feature_idx, (mean, std) in enumerate(params):\n",
    "            x_i = x[feature_idx]\n",
    "            probs[class_value] *= normal_pdf(mean, std)(x_i)  # computes P(X_i | y) for current y = class_value\n",
    "    # get the class that maximize the conditional probability\n",
    "    return max(probs, key=lambda class_value: probs[class_value])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# ======================= TEST:==================================\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def test_get_normal_parameters():\n",
    "    params_by_class = get_distrib_parameters(FEAT, COL_NAMES, LABELS)\n",
    "    print(\"Format: (mean_i, std_i), ...,  for each class\")\n",
    "    pprint(params_by_class)\n",
    "\n",
    "\n",
    "def test_predict_bayes():\n",
    "    params_by_class = get_distrib_parameters(FEAT, COL_NAMES, LABELS)\n",
    "    # test sample\n",
    "    idx = np.random.randint(0, len(FEAT))\n",
    "    x = FEAT.iloc[idx]\n",
    "    print(\"Sample to predict:\\n\", x, \"\\n \")\n",
    "    pred = predict_bayes(x, params_by_class)\n",
    "    print(\"Predicted class: \", pred)\n",
    "    print(\"Actual class: \", LABELS.iloc[idx])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # test_get_normal_parameters()\n",
    "    print(\" \")\n",
    "    test_predict_bayes()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:21:41.657819303Z",
     "start_time": "2024-01-08T21:21:41.654523474Z"
    }
   },
   "id": "b3b8bf1f2bdd4c28",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "`log_reg.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53919941aa5dfa84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def z(X: NDArray, w: NDArray, b: fl) -> fl:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    ``np.dot(X, w) + b``: `float` or `NDArray[float]` (i.e. `floating`)\n",
    "    Notes\n",
    "    -----\n",
    "    `w` and `X` can be interchanged e.g. `z(w, X, b)`, it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z: fl) -> fl:\n",
    "    \"\"\" Returns\n",
    "    -----------\n",
    "    1 / (1 + exp(-z))\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: NDArray):\n",
    "    return (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "\n",
    "def grad(X: NDArray, y: NDArray, w: NDArray, b: fl):\n",
    "    \"\"\"Computes (vectorized) the gradient of the log loss function w.r.t \"w\" and \"b\" for the current iteration.\n",
    "    It is used in the gradient descent algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    Returns\n",
    "    -------\n",
    "    (dw, db) :\n",
    "        The gradient of the log loss function w.r.t \"w\" and \"b\".\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -np.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = np.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -np.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: fl, n_it: int, lr: float) -> tuple[NDArray, fl]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        initial weight vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        inital bias\n",
    "    `n_it` : int\n",
    "        iterations number\n",
    "    `lr` : float\n",
    "        learning rate\n",
    "    Returns\n",
    "    -------\n",
    "        Trained (weight vector, bias) with gradient descent that minimize the log loss function.\"\"\"\n",
    "    return grad_desc_ml(X, y, grad, w, b, lr, n_it)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:21:41.708795389Z",
     "start_time": "2024-01-08T21:21:41.657264797Z"
    }
   },
   "id": "1b681218d41ac429",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample to predict:\n",
      " sepal length    4.6\n",
      "sepal width     3.4\n",
      "petal length    1.4\n",
      "petal width     0.3\n",
      "Name: 6, dtype: float64 \n",
      " \n",
      "Predicted class:  Iris-setosa\n",
      "Actual class:  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "test_predict_bayes()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:21:52.356263482Z",
     "start_time": "2024-01-08T21:21:52.354525013Z"
    }
   },
   "id": "d8e854fe757ad959",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
