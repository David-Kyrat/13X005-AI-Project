{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook created to train the logistic regression model without having to reload the dataset every time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7651970a6e707"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Because for some reason, ucimlrepository takes an weirdly long time to load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9707a8373c00428e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length  sepal width  petal length  petal width            class\n73            6.1          2.8           4.7          1.2  Iris-versicolor\n18            5.7          3.8           1.7          0.3      Iris-setosa\n118           7.7          2.6           6.9          2.3   Iris-virginica\n78            6.0          2.9           4.5          1.5  Iris-versicolor\n76            6.8          2.8           4.8          1.4  Iris-versicolor\n31            5.4          3.4           1.5          0.4      Iris-setosa\n64            5.6          2.9           3.6          1.3  Iris-versicolor\n141           6.9          3.1           5.1          2.3   Iris-virginica\n68            6.2          2.2           4.5          1.5  Iris-versicolor\n82            5.8          2.7           3.9          1.2  Iris-versicolor\n110           6.5          3.2           5.1          2.0   Iris-virginica\n12            4.8          3.0           1.4          0.1      Iris-setosa\n36            5.5          3.5           1.3          0.2      Iris-setosa\n9             4.9          3.1           1.5          0.1      Iris-setosa\n19            5.1          3.8           1.5          0.3      Iris-setosa\n56            6.3          3.3           4.7          1.6  Iris-versicolor\n104           6.5          3.0           5.8          2.2   Iris-virginica\n69            5.6          2.5           3.9          1.1  Iris-versicolor\n55            5.7          2.8           4.5          1.3  Iris-versicolor\n132           6.4          2.8           5.6          2.2   Iris-virginica\n29            4.7          3.2           1.6          0.2      Iris-setosa\n127           6.1          3.0           4.9          1.8   Iris-virginica\n26            5.0          3.4           1.6          0.4      Iris-setosa\n128           6.4          2.8           5.6          2.1   Iris-virginica\n131           7.9          3.8           6.4          2.0   Iris-virginica\n145           6.7          3.0           5.2          2.3   Iris-virginica\n108           6.7          2.5           5.8          1.8   Iris-virginica\n143           6.8          3.2           5.9          2.3   Iris-virginica\n45            4.8          3.0           1.4          0.3      Iris-setosa\n30            4.8          3.1           1.6          0.2      Iris-setosa\n22            4.6          3.6           1.0          0.2      Iris-setosa\n15            5.7          4.4           1.5          0.4      Iris-setosa\n65            6.7          3.1           4.4          1.4  Iris-versicolor\n11            4.8          3.4           1.6          0.2      Iris-setosa\n42            4.4          3.2           1.3          0.2      Iris-setosa\n146           6.3          2.5           5.0          1.9   Iris-virginica\n51            6.4          3.2           4.5          1.5  Iris-versicolor\n27            5.2          3.5           1.5          0.2      Iris-setosa\n4             5.0          3.6           1.4          0.2      Iris-setosa\n32            5.2          4.1           1.5          0.1      Iris-setosa\n142           5.8          2.7           5.1          1.9   Iris-virginica\n85            6.0          3.4           4.5          1.6  Iris-versicolor\n86            6.7          3.1           4.7          1.5  Iris-versicolor\n16            5.4          3.9           1.3          0.4      Iris-setosa\n10            5.4          3.7           1.5          0.2      Iris-setosa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length</th>\n      <th>sepal width</th>\n      <th>petal length</th>\n      <th>petal width</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>73</th>\n      <td>6.1</td>\n      <td>2.8</td>\n      <td>4.7</td>\n      <td>1.2</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>5.7</td>\n      <td>3.8</td>\n      <td>1.7</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>7.7</td>\n      <td>2.6</td>\n      <td>6.9</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>6.0</td>\n      <td>2.9</td>\n      <td>4.5</td>\n      <td>1.5</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>6.8</td>\n      <td>2.8</td>\n      <td>4.8</td>\n      <td>1.4</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>5.4</td>\n      <td>3.4</td>\n      <td>1.5</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>5.6</td>\n      <td>2.9</td>\n      <td>3.6</td>\n      <td>1.3</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>6.9</td>\n      <td>3.1</td>\n      <td>5.1</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>6.2</td>\n      <td>2.2</td>\n      <td>4.5</td>\n      <td>1.5</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>5.8</td>\n      <td>2.7</td>\n      <td>3.9</td>\n      <td>1.2</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>6.5</td>\n      <td>3.2</td>\n      <td>5.1</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4.8</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>5.5</td>\n      <td>3.5</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4.9</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>5.1</td>\n      <td>3.8</td>\n      <td>1.5</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>6.3</td>\n      <td>3.3</td>\n      <td>4.7</td>\n      <td>1.6</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.8</td>\n      <td>2.2</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>5.6</td>\n      <td>2.5</td>\n      <td>3.9</td>\n      <td>1.1</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>5.7</td>\n      <td>2.8</td>\n      <td>4.5</td>\n      <td>1.3</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>6.4</td>\n      <td>2.8</td>\n      <td>5.6</td>\n      <td>2.2</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>6.1</td>\n      <td>3.0</td>\n      <td>4.9</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>5.0</td>\n      <td>3.4</td>\n      <td>1.6</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>6.4</td>\n      <td>2.8</td>\n      <td>5.6</td>\n      <td>2.1</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>7.9</td>\n      <td>3.8</td>\n      <td>6.4</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>6.7</td>\n      <td>2.5</td>\n      <td>5.8</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>6.8</td>\n      <td>3.2</td>\n      <td>5.9</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>4.8</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>4.8</td>\n      <td>3.1</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>4.6</td>\n      <td>3.6</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5.7</td>\n      <td>4.4</td>\n      <td>1.5</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>6.7</td>\n      <td>3.1</td>\n      <td>4.4</td>\n      <td>1.4</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4.8</td>\n      <td>3.4</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>4.4</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>6.4</td>\n      <td>3.2</td>\n      <td>4.5</td>\n      <td>1.5</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>5.2</td>\n      <td>3.5</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>5.2</td>\n      <td>4.1</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>5.8</td>\n      <td>2.7</td>\n      <td>5.1</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>6.0</td>\n      <td>3.4</td>\n      <td>4.5</td>\n      <td>1.6</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>6.7</td>\n      <td>3.1</td>\n      <td>4.7</td>\n      <td>1.5</td>\n      <td>Iris-versicolor</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.3</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5.4</td>\n      <td>3.7</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "from numpy import floating as fl, float32 as f32, float64 as f64, int32 as i32\n",
    "from numpy.typing import NDArray\n",
    "from pprint import pprint\n",
    "\n",
    "# Iris dataset\n",
    "DATASET_ID = 53\n",
    "\n",
    "iris = fetch_ucirepo(id=DATASET_ID)  # fetch dataset\n",
    "assert iris.data is not None\n",
    "\n",
    "DATA: DataFrame = iris.data.original\n",
    "LAB_NAME: str = iris.data[\"headers\"][-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "FEAT, FEAT_test, y_train, y_test = train_test_split(iris.data.features, DATA[LAB_NAME], test_size=0.3, random_state=42)\n",
    "\n",
    "DATA_train = FEAT.copy(deep=True)\n",
    "DATA_train[\"class\"] = y_train\n",
    "\n",
    "DATA_test = FEAT_test.copy(deep=True)\n",
    "DATA_test[\"class\"] = y_test\n",
    "\n",
    "# FEAT: DataFrame = X_train\n",
    "# FEAT_test: DataFrame = X_test\n",
    "\n",
    "LABELS_STR: DataFrame = DATA_train[LAB_NAME]  # type: ignore\n",
    "LABELS_STR_test: DataFrame = DATA_test[LAB_NAME]  # type: ignore\n",
    "\n",
    "lab_values  = LABELS_STR.unique()\n",
    "lab_values_test  = LABELS_STR_test.unique()\n",
    "\n",
    "LAB_IDX_VAL: dict[int, str] = dict(zip(range(len(lab_values)), lab_values))\n",
    "LAB_VAL_IDX: dict[str, int] = dict(zip(lab_values, range(len(lab_values))))\n",
    "\n",
    "LAB_IDX_VAL_test: dict[int, str] = dict(zip(range(len(lab_values_test)), lab_values_test))\n",
    "LAB_VAL_IDX_test: dict[str, int] = dict(zip(lab_values_test, range(len(lab_values_test))))\n",
    "\n",
    "\n",
    "LABELS: NDArray[int] = np.array([LAB_VAL_IDX[class_value] for class_value in LABELS_STR])\n",
    "LABELS_test: NDArray[int] = np.array([LAB_VAL_IDX[class_value] for class_value in LABELS_STR])\n",
    "COL_NAMES = list(FEAT.columns)\n",
    "DATA_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T23:17:20.530791744Z",
     "start_time": "2024-01-08T23:17:18.998009376Z"
    }
   },
   "id": "863f7e3ebaac994f",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "`gradient_descent.py`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7140fc85d73feb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def grad_desc_ml(\n",
    "        features: NDArray, labels: NDArray, df, w: NDArray, b: fl, alpha: float, num_iters: int\n",
    ") -> tuple[NDArray, fl]:\n",
    "    \"\"\"Same gradient descent `gradient_desent` method, but that takes `features` (X) and `labels` (y)\n",
    "    as additional parameters, since they're obviously going to be need for any kind of learning whatsoever.\n",
    "    Parameters\n",
    "    ----------\n",
    "    `features` : NDArray\n",
    "        Samples / features.\n",
    "    `labels` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `df`: function\n",
    "        derivative function (i.e. gradient)\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    `alpha`: float\n",
    "        define how the function will converge. Values too big will give bad results and values too small won't converge or will converge too slowly\n",
    "    `num_iters`: Number of iterations\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal vector for the initial configuration and parameters\"\"\"\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        grad_w, grad_b = df(features, labels, w, b)\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "    return w, b\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T23:09:41.129609967Z"
    }
   },
   "id": "97d4dc32e4b35c04",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "`log_reg.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53919941aa5dfa84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def z(X: NDArray, w: NDArray, b: fl) -> fl:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    ``np.dot(X, w) + b``: `float` or `NDArray[float]` (i.e. `floating`)\n",
    "    Notes\n",
    "    -----\n",
    "    `w` and `X` can be interchanged e.g. `z(w, X, b)`, it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z: fl) -> fl:\n",
    "    \"\"\" Returns\n",
    "    -----------\n",
    "    1 / (1 + exp(-z))\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: NDArray):\n",
    "    return (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "\n",
    "def grad(X: NDArray, y: NDArray, w: NDArray, b: fl):\n",
    "    \"\"\"Computes (vectorized) the gradient of the log loss function w.r.t \"w\" and \"b\" for the current iteration.\n",
    "    It is used in the gradient descent algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        weights vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        bias\n",
    "    Returns\n",
    "    -------\n",
    "    (dw, db) :\n",
    "        The gradient of the log loss function w.r.t \"w\" and \"b\".\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -np.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = np.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -np.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: fl, n_it: int, lr: float) -> tuple[NDArray, fl]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    `X` : NDArray\n",
    "        Samples / features.\n",
    "    `y` : NDArray\n",
    "        labels / class associated to each sample.\n",
    "    `w` : NDArray\n",
    "        initial weight vector.\n",
    "    `b` : fl (float or NDArray[float])\n",
    "        inital bias\n",
    "    `n_it` : int\n",
    "        iterations number\n",
    "    `lr` : float\n",
    "        learning rate\n",
    "    Returns\n",
    "    -------\n",
    "        Trained (weight vector, bias) with gradient descent that minimize the log loss function.\"\"\"\n",
    "    return grad_desc_ml(X, y, grad, w, b, lr, n_it)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T23:09:41.130080646Z",
     "start_time": "2024-01-08T23:09:41.129828247Z"
    }
   },
   "id": "1b681218d41ac429",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression but with CuPy (Nvidia / Cuda)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596326ec80a95575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(data, predicted_values):\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "    \"\"\" This function calculates the performance metrics for each class in a binary classification problem.\n",
    "        The metrics calculated are Precision, Recall, and F1 Score.\n",
    "        :param data: (DataFrame): The DataFrame containing the actual labels.\n",
    "        :param predicted_values: (list): The list containing the predicted labels.\n",
    "        :return: dict: A dictionary containing the performance metrics for each class.\"\"\"\n",
    "    y_true, y_pred = data, predicted_values\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_score': float(f1)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T23:09:41.129918776Z"
    }
   },
   "id": "8b450001d36befda",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cupy import ndarray as CPArray\n",
    "from numpy.random import rand, randint\n",
    "\n",
    "\n",
    "def z(w: CPArray, X: CPArray, b: float) -> CPArray:\n",
    "    \"\"\":return: ``cp.dot(X, w) + b``.\n",
    "    i.e. float or CPArray[float] (i.e. cupy.ndarray)\n",
    "    NOTE: `w` and `X` can be interchanged e.g. (`z(X, w, b)), it won't give\n",
    "    the same result (in general) but as long as matrix multiplication dimensions\n",
    "    are respected, it will work.\"\"\"\n",
    "    return cp.dot(X, w) + b\n",
    "\n",
    "\n",
    "def sigmoid(z): return 1 / (1 + cp.exp(-z))\n",
    "\n",
    "\n",
    "def norm(X: CPArray): return (X - cp.mean(X)) / cp.std(X)\n",
    "\n",
    "\n",
    "def grad(X: CPArray, y: CPArray, w: CPArray, b: float):\n",
    "    \"\"\":return: (dw, db). i.e. Computes aforementioned derivatives w.r.t \"w\" and \"b\". \n",
    "    (on gpu. X, y, w, b are `cupy.ndarray` shortened to `CPArray`)\"\"\"\n",
    "\n",
    "    predictions = sigmoid(z(w, X, b))  # Sigmoid function applied to z\n",
    "    errors = y - predictions  # Difference between actual and predicted values\n",
    "    db = -cp.sum(errors)  # Vectorized computation of db component\n",
    "\n",
    "    X_sum_over_rows = cp.sum(X, axis=1)  # Sum over rows of X\n",
    "    dw = -cp.sum(X_sum_over_rows * errors)  # Vectorized computation of dw component\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# because shorter names for function and variable, while keeping the function name as asked from the exercise\n",
    "def train_log_reg(X: NDArray, y: NDArray, w: NDArray, b: float, n_it: int, lr: float) -> tuple[NDArray, float]:\n",
    "    \"\"\"\n",
    "    :param X: Feature matrix (covariables)\n",
    "    :param y: Label vector\n",
    "    :param w: initial weight vector\n",
    "    :param b:  initial bias\n",
    "    :param n_it: iterations number\n",
    "    :param lr: learning rate\n",
    "    :return: Trained weight vector and bias to minimize by gradient descent.\n",
    "    \"\"\"\n",
    "    X, y, w = map(cp.array, (X, y, w))\n",
    "    for _ in range(n_it):\n",
    "        grad_w, grad_b = grad(X, y, w, b)\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def predict_log_reg(X: NDArray, w: NDArray, b):\n",
    "    \"\"\" Predict the class labels for a set of examples X using logistic regression parameters w and b.\n",
    "    :param X: The input features. 2D Matrix NDArray\n",
    "    :param w: The weights of the logistic regression model. Vector NDArray\n",
    "    :param b: The bias of the logistic regression model. float\n",
    "    :return: Vector of predicted class labels (0 or 1) for each example in X. Vector NDArray\n",
    "    \"\"\"\n",
    "    X, w = map(cp.array, (X, w))\n",
    "    return i32(sigmoid(z(w, X, b)).get() >= 0.5)\n",
    "\n",
    "\n",
    "def test_train_gpu(m, n):\n",
    "    X, y, w, b = rand(m, n), rand(m), rand(n), rand()\n",
    "    n_it, lr = 100, 0.03\n",
    "    w, b = train_log_reg(X, y, w, b, n_it, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T23:09:41.129976082Z"
    }
   },
   "id": "8fd76d66732d0098",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m, n = FEAT.shape\n",
    "init_w = np.random.rand(n)\n",
    "init_b = np.random.rand()\n",
    "n_it, lr = 10000, 1e-10\n",
    "\n",
    "train_log_reg(FEAT.to_numpy(), LABELS, init_w, init_b, n_it, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T23:09:41.130026634Z"
    }
   },
   "id": "b916e84f34b77584",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pred_compute(ta_tes, w, b):\n",
    "    \n",
    "    # X_test = np.array([data_test.Gender_Female, data_test.Age, data_test.EstimatedSalary]).T  # features\n",
    "    # X_test[:, 1:3] = np.apply_along_axis(norm, 0, X_test[:, 1:3])\n",
    "    predicted_val_logreg = predict_log_reg(FEAT_test, w, b)\n",
    "    metrics = compute_metrics(predicted[\"actual\"], predicted_val_logreg)\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T23:09:41.131364078Z",
     "start_time": "2024-01-08T23:09:41.130173272Z"
    }
   },
   "id": "b0b4441bfe52cd68",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def maximize_train_param(tries: int):\n",
    "    params = []\n",
    "    # w = np.array([0.13276234, 0.24566002, - 0.00970713])\n",
    "    w = np.array([0.13017195, 0.24306963, -0.01229752])\n",
    "    n_it = 3900\n",
    "    # for n_it in np.linspace(3732, 3760, tries, dtype=int):\n",
    "    print(pred_compute(data_test, w, 0)[\"f1_score\"])\n",
    "    for lr in np.linspace(1e-9 * 0.01, 1e-9 * 2, tries, dtype=f64):\n",
    "        # w = np.random.uniform(0, 0.5, size=3)\n",
    "        b = 0.\n",
    "        w, b = train_log_reg(X, labels, np.array([0.13017195, 0.24306963, -0.01229752]), 0., n_it, lr)\n",
    "        f1_score_ = pred_compute(data_test, w, b)[\"f1_score\"]\n",
    "        params.append((w, n_it, lr, f1_score_))\n",
    "        print(w, lr, \"\\nf1_score:\", f1_score_)\n",
    "        print(\"______\")\n",
    "\n",
    "    return max(params, key=lambda x: x[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T23:09:41.132236910Z"
    }
   },
   "id": "aa22b58e0d6cd603",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
