@inreference{RegressionLogistique2023,
    title = {Régression logistique},
    booktitle = {Wikipédia},
    date = {2023-12-12T14:52:00Z},
    url = {
           https://fr.wikipedia.org/w/index.php?title=R%C3%A9gression_logistique&oldid=210479759
           },
    urldate = {2024-01-05},
    abstract = {En statistiques, la régression logistique ou modèle logit est un
                modèle de régression binomiale. Comme pour tous les modèles de
                régression binomiale, il s'agit d'expliquer au mieux une variable
                binaire (la présence ou l'absence d'une caractéristique donnée)
                par des observations réelles nombreuses, grâce à un modèle
                mathématique. En d'autres termes d'associer une variable
                aléatoire de Bernoulli (génériquement notée y \{\textbackslash
                displaystyle y\} ) à un vecteur de variables aléatoires ( x 1 , …
                , x K ) \{\textbackslash displaystyle (x\_\{1\},\textbackslash
                ldots ,x\_\{K\})\} . La régression logistique constitue un cas
                particulier de modèle linéaire généralisé. Elle est largement
                utilisée en apprentissage automatique.},
    langid = {french},
    annotation = {Page Version ID: 210479759},
}

@online{ScipyOptimizeFmin,
    title = {Scipy.Optimize.Fmin — {{SciPy}} v1.11.4 {{Manual}}},
    url = {
           https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html#scipy.optimize.fmin
           },
}
@online{LinearModels,
    title = {1.1. {{Linear Models}}},
    shorttitle = {Sklearn\_tuto\_logistic\_regression},
    url = {https://scikit-learn/stable/modules/linear_model.html},
    abstract = {The following are a set of methods intended for regression in
                which the target value is expected to be a linear combination of
                the features. In mathematical notation, if\textbackslash hat\{y\}
                is the predicted val...},
    langid = {english},
    organization = {{scikit-learn}},
    file = {/home/noahl/Zotero/storage/AVSNPPMK/linear_model.html},
}

@inreference{ModeleLineaireGeneralise2022,
    title = {Modèle linéaire généralisé},
    booktitle = {Wikipédia},
    date = {2022-01-22T15:14:48Z},
    url = {
           https://fr.wikipedia.org/w/index.php?title=Mod%C3%A8le_lin%C3%A9aire_g%C3%A9n%C3%A9ralis%C3%A9&oldid=190125578
           },
    urldate = {2024-01-05},
    abstract = {En statistiques, le modèle linéaire généralisé (MLG) souvent
                connu sous les initiales anglaises GLM est une généralisation
                souple de la régression linéaire. Le GLM généralise la régression
                linéaire en permettant au modèle linéaire d'être relié à la
                variable réponse via une fonction lien et en autorisant
                l'amplitude de la variance de chaque mesure d'être une fonction
                de sa valeur prévue, en fonction de la loi choisie. Formellement,
                E ⁡ ( Y | X ) = μ = g − 1 ( X β ) \{\textbackslash displaystyle
                \textbackslash operatorname \{E\} (Y|\textbackslash mathbf \{X\}
                )=\{\textbackslash boldsymbol \{\textbackslash mu \}\}=g\^\{-1\}(
                \textbackslash mathbf \{X\} \{\textbackslash boldsymbol \{
                \textbackslash beta \}\})\} où E ⁡ ( Y | X ) \{\textbackslash
                displaystyle \textbackslash operatorname \{E\} (Y|\textbackslash
                mathbf \{X\} )\} est l'espérance mathématique de Y \{
                \textbackslash displaystyle Y\} conditionnelle à X \{
                \textbackslash displaystyle \textbackslash mathbf \{X\} \} ; X β
                \{\textbackslash displaystyle \textbackslash mathbf \{X\} \{
                \textbackslash boldsymbol \{\textbackslash beta \}\}\} est le
                prédicteur linéaire, c'est-à-dire une combinaison linéaire des
                variables explicatives, et où g \{\textbackslash displaystyle g\}
                est une fonction monotone appelée fonction de lien. De plus Var ⁡
                ( Y | X ) = V ⁡ ( μ ) = V ⁡ ( g − 1 ( X β ) ) . \{\textbackslash
                displaystyle \textbackslash operatorname \{Var\} (Y|
                \textbackslash mathbf \{X\} )=\textbackslash operatorname \{V\} (
                \{\textbackslash boldsymbol \{\textbackslash mu \}\})=
                \textbackslash operatorname \{V\} (g\^\{-1\}(\textbackslash
                mathbf \{X\} \{\textbackslash boldsymbol \{\textbackslash beta \}
                \})).\} où V \{\textbackslash displaystyle \textbackslash
                operatorname \{V\} \} est appelée fonction variance, qui dépend
                la loi Y \{\textbackslash displaystyle Y\} (au sein de la famille
                exponentielle) La théorie des modèles linéaires généralisés a été
                formulée par John Nelder et Robert Wedderburn (en) comme un moyen
                d'unifier les autres modèles statistiques y compris la régression
                linéaire, la régression logistique et la régression de Poisson.
                Ils proposent une méthode itérative dénommée méthode des moindres
                carrés repondérés itérativement (en) pour l'estimation du maximum
                de vraisemblance des paramètres du modèle. L'estimation du
                maximum de vraisemblance reste populaire et est la méthode par
                défaut dans de nombreux logiciels de calculs statistiques.
                D'autres approches incluant les statistiques bayésiennes et la
                méthode des moindres carrés convenant aux réponses à variance
                stabilisées, ont été développées.},
    langid = {french},
    annotation = {Page Version ID: 190125578},
    file = {/home/noahl/Zotero/storage/LBGALXCS/Modèle_linéaire_généralisé.html},
}

@inreference{ClassificationNaiveBayesienne2022,
    title = {Classification naïve bayésienne},
    booktitle = {Wikipédia},
    date = {2022-12-05T09:25:49Z},
    url = {
           https://fr.wikipedia.org/w/index.php?title=Classification_na%C3%AFve_bay%C3%A9sienne&oldid=199241198
           },
    abstract = {La classification naïve bayésienne est un type de classification
                bayésienne probabiliste simple basée sur le théorème de Bayes
                avec une forte indépendance (dite naïve) des hypothèses. Elle met
                en œuvre un classifieur bayésien naïf, ou classifieur naïf de
                Bayes, appartenant à la famille des classifieurs linéaires. Un
                terme plus approprié pour le modèle probabiliste sous-jacent
                pourrait être « modèle à caractéristiques statistiquement
                indépendantes ». En termes simples, un classifieur bayésien naïf
                suppose que l'existence d'une caractéristique pour une classe,
                est indépendante de l'existence d'autres caractéristiques. Un
                fruit peut être considéré comme une pomme s'il est rouge, arrondi
                , et fait une dizaine de centimètres. Même si ces
                caractéristiques sont liées dans la réalité, un classifieur
                bayésien naïf déterminera que le fruit est une pomme en
                considérant indépendamment ces caractéristiques de couleur, de
                forme et de taille. Selon la nature de chaque modèle probabiliste
                , les classifieurs bayésiens naïfs peuvent être entraînés
                efficacement dans un contexte d'apprentissage supervisé. Dans
                beaucoup d'applications pratiques, l'estimation des paramètres
                pour les modèles bayésiens naïfs repose sur le maximum de
                vraisemblance. Autrement dit, il est possible de travailler avec
                le modèle bayésien naïf sans se préoccuper de probabilité
                bayésienne ou utiliser les méthodes bayésiennes. Malgré leur
                modèle de conception « naïf » et ses hypothèses de base
                extrêmement simplistes, les classifieurs bayésiens naïfs ont fait
                preuve d'une efficacité plus que suffisante dans beaucoup de
                situations réelles complexes. En 2004, un article a montré qu'il
                existe des raisons théoriques derrière cette efficacité
                inattendue. Toutefois, une autre étude de 2006 montre que des
                approches plus récentes (arbres renforcés, forêts aléatoires)
                permettent d'obtenir de meilleurs résultats. L'avantage du
                classifieur bayésien naïf est qu'il requiert relativement peu de
                données d'entraînement pour estimer les paramètres nécessaires à
                la classification, à savoir moyennes et variances des différentes
                variables. En effet, l'hypothèse d'indépendance des variables
                permet de se contenter de la variance de chacune d'entre elles
                pour chaque classe, sans avoir à calculer de matrice de
                covariance.},
    langid = {french},
    annotation = {Page Version ID: 199241198},
}


@inreference{ClassifieurLineaire2022,
    title = {Classifieur linéaire},
    booktitle = {Wikipédia},
    date = {2022-01-03T13:54:45Z},
    url = {
           https://fr.wikipedia.org/w/index.php?title=Classifieur_lin%C3%A9aire&oldid=189518969
           },
    urldate = {2024-01-05},
    abstract = {En apprentissage automatique, les classifieurs linéaires sont
                une famille d'algorithmes de classement statistique. Le rôle d'un
                classifieur est de classer dans des groupes (des classes) les
                échantillons qui ont des propriétés similaires, mesurées sur des
                observations. Un classifieur linéaire est un type particulier de
                classifieur, qui calcule la décision par combinaison linéaire des
                échantillons.},
    langid = {french},
    annotation = {Page Version ID: 189518969},
}



